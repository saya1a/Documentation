**1. what is Poll SCM in Jenkins?**
***********************************
In Jenkins, Poll SCM means that Jenkins periodically checks the version control system (VCS) for any changes. When changes are detected, it triggers a build automatically. This feature allows you to schedule how often Jenkins checks for updates, making it an integral part of continuous integration (CI) processes. By configuring Poll SCM, you can ensure that your codebase is frequently checked for updates, leading to timely builds, automated responses to code changes, and seamless integration with scheduled tasks.
******************
**Q. What are the types of build triggers in Jenkins?**
********************************
In Jenkins, build triggers are mechanisms that initiate the execution of a job. They help automate the build process by defining specific conditions under which a build should start.
1. Mannual trigger:  Users can manually start a build by clicking the "Build Now" button in the Jenkins web interface.
2. Poll SCM:  Jenkins periodically checks the source code repository (e.g., Git, SVN) for changes. If changes are detected, it triggers a build.
3. Build periodically:  Jenkins can schedule builds at specific times using a cron-like syntax.
4. GitHub Hook Trigger for GITScm Polling:  Jenkins can listen for webhooks from GitHub. When changes are pushed to the repository, the webhook triggers a build.
5. Build after Other Projects are Built (Upstream/Downstream Builds): Jenkins can trigger a build when another specified project is built by Specifing the upstream projects in the "Build after other projects are built" section.
6. Trigger build Remotely: You can trigger Jenkins builds remotely via an HTTP URL. Enable "Trigger builds remotely" and provide a unique token.
7. Parameterized Builds: Allows you to trigger builds with specific parameters.
***************************************
**Q. What is continuous delivery and continuous deployment?**
*****************************
Continuous Delivery is software development practice that focuses on automating the process of delivering code changes to production-like environments after passing through entire pipeline of build, test and deployment.

In continuous delivery, the deployment to the production environment is not automated. instead, it requires a manual trigger or approval process.

CD allows for human intervention and decision making before deploying code to the production environment. it allows teams to assess the changes, perform final testing, and ensure that business requirements are met.

Continuous Delivery is often chosen in scenarios where organizations want to achieve a balance between rapid development and the need for human validation before releasing changes to customers. it reduces the risk of expected issues in production.

continuous deployment is an extension to the continuous delivery. it is a practice where code changes that pass automated tests are automatically and immediately deployed to the production environments without requiring manual intervention or approval.

CD eliminates the need for human intervention or approval in the production deployment process.  if the automated tests pass, the code goes live.

Continuous Deployment is often implemented by organizations that prioritize rapid delivery of new features and bug fixes to end-users. it is common in environments where there is strong focus on continuous improvement and automation.
***************************
**Q. explain build lifecycle in Jenkins?**
***************************
Job Configuration: Set up the job, configure SCM, and define build triggers.

Build Execution: Checkout source code, execute build steps, and run tests.

Post-Build Actions: Archive artifacts, deploy the application, and send notifications.

Build Monitoring: Monitor build progress and status.

Build Analysis: Analyze build history, test results, and code coverage.

Build Cleanup: Clean up the workspace and old artifacts.
*********************************
**Q. What are shared libraries in Jenkins?**
**********************************
In Jenkins, a shared library is a way to store commonly used code(reusable code), such as scripts or functions, that can be used by different Jenkins pipelines.

Instead of writing the same code again and again in multiple pipelines, you can create a shared library and use it in all the pipelines that need it. This can make your code more organized and easier to maintain.

An organization has several microservices, each with its own codebase and repository. However, the build, test, and deployment steps for these microservices are quite similar. Instead of duplicating the pipeline code across multiple repositories, the organization can use Jenkins shared libraries to centralize and reuse common pipeline logic.
*********************
**Q. explain the difference between Declararive and scripted pipeline in Jenkins?**
*****************************
Declarative: Declarative is a more recent and advanced implementation of a pipeline as a code, provide a more straightforward and readable syntax for defining CI/CD pipelines. They are designed to be user-friendly and enforce a structured approach to pipeline definition.

The syntax is more straightforward and easier to understand, making it accessible for users with minimal programming experience.

Uses a pipeline block as the root element.

Declarative Pipelines come with built-in error handling and post-build actions, such as post blocks to define actions like always, success, failure, and unstable.

Uses structured blocks like stages, steps, and environment to organize the pipeline.

The syntax is validated before execution, catching syntax errors early.


Scripted: Scripted was the first and traditional implementation of the pipeline as a code in Jenkins. It was designed as a general-purpose DSL (Domain Specific Language) built with Groovy. 
They are suitable for more complex pipelines and scenarios where advanced scripting is required.
Syntax errors are only caught during execution, not beforehand.
*************************
**Q. How do you handle the build failure or what is the procedure you follow when build failed in Jenkins?**
***************************
When a Jenkins build fails, I follow a structured debugging process to quickly identify and resolve the issue. My approach includes like..

The first thing I do is check the console logs (Build â†’ Console Output) to find error messages.
I look for keywords like syntax errors, dependency failures, missing files, or permission issues.

I check which stage failed:

Build Stage â†’ Compilation errors (Go, Java, Python, etc.).

Test Stage â†’ Unit test failures.

SonarQube/Aqua Trivy â†’ Code quality or security scan failures.

Artifact Upload â†’ Nexus/S3 permission issues.

Deployment Stage â†’ Kubernetes/EKS issues.

I  Check the Recent Code Changes, If the build was working fine before, I compare the latest commits using: git diff HEAD~1 HEAD
If a recent commit introduced a bug, I discuss it with the developer or rollback the change.

I Check Jenkins Agent Issues by Verifying the Jenkins Slave/Agent Logs: tail -n 100 /var/log/jenkins/jenkins.log
If the agent is disconnected, I restart the Jenkins node:

Check Dependencies & Environment Issues
Check if any new package/library update broke the build
mvn dependency:tree  # For Java
go list -m all       # For Golang
npm list            # For Node.js
Verify environment variables are set correctly:
printenv | grep DB_

Check if the Docker container has the correct files:
I re-run the build with debugging enabled:
mvn clean install -X  # For Java
go test -v            # For Golang
npm run debug         # For Node.js

Notify the Team & Document the Issue
If the issue is code-related, I notify the developer via Slack/Teams.
If it's an infrastructure issue, I escalate to the relevant team.
I document the issue in Confluence or Jira for future reference.

Implement a Fix & Prevent Future Failures

ðŸ”¹ If a missing dependency caused the failure, I update the Dockerfile or Jenkinsfile.

ðŸ”¹ If tests failed, I work with the team to fix them.

ðŸ”¹ If infrastructure was the issue, I add monitoring & alerting (Prometheus, Grafana, ELK).
*******************************
**Q. Explain the concept of blue green deployment how you can achieve it with jenkins?**
***********************************

Blue-Green Deployment is a zero-downtime deployment strategy where two identical environments (Blue and Green) exist. At any time:

Blue is the live (production) environment.

Green is the new version being tested.

Once the Green version is fully tested and verified, traffic is switched from Blue â†’ Green, making Green the new production environment.

Jenkins can automate Blue-Green Deployment for applications running on Docker, Kubernetes, or EC2-based environments.

If using Kubernetes, have two namespaces (blue and green).

If using Docker, run two container instances (blue-app and green-app).

If using AWS, have two EC2 instances with an Elastic Load Balancer (ELB).

Create two identical environments (Blue and Green) with the same infrastructure and configurations

Set up a load balancer or DNS configuration that can direct traffic to either the Blue or Green environment. Initially, all traffic should be directed to the Blue environment

Open Jenkins and create a new job or configure an existing job for your application deployment.

Set up Source Code Management (e.g., Git) and provide the repository URL.

Add build steps for building your application.

Add deployment steps to deploy the application to the Blue environment

In your Jenkins job or deployment script, introduce logic to switch traffic between the Blue and Green environments.

If using a load balancer, update the load balancer configuration to route traffic to the Green environment.

If using DNS, update the DNS records to point to the Green environment

Run the Jenkins job manually or trigger it based on webhook, SCM polling, or other triggers.

Observe the deployment to the Blue environment.

Trigger the Blue-Green switch, either manually or automatically, depending on your implementation

Monitor the deployment and verify that the application is functioning correctly in the Green environment.

If issues are detected, roll back the deployment by switching traffic back to the Blue environment
*******************
**Q. How do you secure your jenkins?**
*********************
Jenkins gives a few options enabling authentication mechnisims like LDAP, Active Directory, or OAuth, which can be used to secure your jenkins so that only authenticated users are accessing Jenkins. so that we can enable our authnetication mechanism, so that only trusted users are accessing your jenkins instance. access control can also be configured using role based security which basically defines the permissions for your users and groups, so the LDAP user or the AD user or the OAuth users you can further control the permission by making use of your role bases security. 
Additionally all your Jenkins connection be encrypted using https.



*********
**Q. How do you use condional logic in a jenkins pipeline?**
***************************
Using conditional logic in Jenkins pipelines allows you to control the flow of your pipeline based on certain conditions. Both Declarative and Scripted pipelines support conditional logic, but they handle it differently.

Jenkins provides the when directive to conditionally execute stages based on expressions, parameters, or environment variables.

Using script {} Block in Declarative Pipelines

Conditional Logic in Scripted Pipelines
For more flexibility, use if-else inside a scripted pipeline
********************
**Q. How do you set up notifications in Jenkins (e.g. email, Slack)**
************************
To set up email notifications in Jenkins, you first need to install the Email Extension Plugin (email-ext) and configure SMTP settings under Manage Jenkins â†’ Configure System.

Depending on your email provider, you need to specify the SMTP server, port, and authentication details. For example, if using Gmail, the SMTP server is smtp.gmail.com, with port 465 (SSL) or 587 (TLS), requiring an app password for authentication. Once configured, you can add email notifications to your Jenkinsfile. A basic implementation involves using the emailext function inside the post block to send notifications upon build success or failure.

For example, if a build fails, an email can be sent to the team with details such as the job name, build number, and a link to the console logs.

Advanced configurations allow attaching build logs, sending emails only when specific stages fail, or using predefined email lists like $DEFAULT_RECIPIENTS to avoid hardcoding addresses.

Additionally, you can conditionally send emails for deployment failures by wrapping deployment steps inside a try-catch block. By integrating email notifications effectively, teams stay informed about critical build and deployment statuses without unnecessary spam.

**Q. How do you optimize the Jenkins pipeline?**

Optimizing a Jenkins pipeline is crucial for improving build speed, resource utilization, and reliability.

Sequential stages can significantly slow down the pipeline, leading to increased build times and potential bottlenecks. Instead, by leveraging parallel execution of independent tasks, we can speed up the overall process. This approach not only accelerates execution but also optimizes resource utilization, effectively reducing the build time and enhancing productivity."

Implementing caching mechanisms is crucial for optimizing build performance. By avoiding redundant downloads of dependencies, artifacts, and Docker layers, caching not only accelerates the build process but also reduces network bandwidth usage and minimizes build time. This strategic approach ensures that previously retrieved and compiled components are efficiently reused, leading to more streamlined and faster builds.

pipeline {
    agent any
    environment {
        MAVEN_CACHE = "${HOME}/.m2/repository"
    }
    stages {
        stage('Build') {
            steps {
                sh 'mvn -Dmaven.repo.local=$MAVEN_CACHE clean package'
            }
        }
    }
}

Moreover, Docker build optimizations, including multi-stage builds and layer caching, play a pivotal role in minimizing image size and accelerating build time. Docker multi-stage builds allow you to create more efficient and leaner Docker images by separating build and runtime dependencies, while layer caching leverages previously built layers to avoid unnecessary rebuilds, ultimately leading to faster and more reliable deployments.

Implement incremental builds to avoid rebuilding the entire project when only a few files have changed. Utilize Git's checkout scm and Jenkins Multibranch Pipeline to prevent unnecessary recompilation of unchanged code.

Duplicated code across pipelines increases maintenance overhead. To avoid this, we can store reusable pipeline logic in Jenkins Shared Libraries. This approach improves maintainability and consistency across projects, allowing for streamlined updates and reducing the risk of errors.

Ensure the pipeline runs on specific agents and efficiently allocate resources. Additionally, run the pipeline on powerful machines to avoid bottlenecks and maintain optimal performance.

Jenkins frequently checking for Git changes can significantly increase the server load. To mitigate this, you can use webhooks instead of polling to trigger builds. Webhooks provide a more efficient mechanism, reducing unnecessary Jenkins load and speeding up build triggers.

Implement the smart test execution, while running all tests every time slow down the pieline. to avoid run only impacted tests using tools like pytest, TestNG, or Gradle.

**Q. How you can ensure the dokcer image consistence and reliable across the environments?**

Ensuring Docker image consistency and reliability across different environments (Dev, SIT, QA, Prod) is crucial to maintaining stability and avoiding unexpected failures.

Use Multi-Stage Builds in Dockerfile to create lightweight, optimized, and consistent images by separating the build and runtime stages. This approach ensures that all environments use the same built binary, promoting consistency and reliability.

Push images to a central repository such as AWS ECR, Nexus, JFrog Artifactory, or Docker Hub to maintain consistency across environments. By ensuring all environments pull the same versioned image, you can avoid local variations and ensure uniformity in deployments.

Avoid using latest tags, as they can lead to unpredictable behavior when environments pull different versions of the image. Instead, use semantic versioning or commit SHA-based tags to ensure every environment uses the exact same image version. This practice promotes consistency and reliability in deployments.

Run vulnerability scanning with Trivy, Aqua Security, or Snyk before pushing images. Prevents security vulnerabilities from propagating across environments.

Run vulnerability scanning with tools like Trivy, Aqua Security, or Snyk before pushing images. This practice helps to prevent security vulnerabilities from propagating across environments, ensuring a more secure deployment process.

Use docker-compose.yml to standardize environment variables and dependencies for local development. This ensures that local development mirrors other environments, promoting consistency and reducing the chances of discrepancies.

Instead of hardcoding configurations in the Docker image, use ConfigMaps and Secrets. This approach keeps images consistent while allowing for environment-specific configurations. It promotes flexibility and security by separating configuration data from the application code and ensuring that sensitive information is managed securely.

Ensure that only tested images are promoted to higher environments by using tools like Jenkins or GitHub Actions. These tools can help automate the promotion process, ensuring that an image is promoted across environments only after it has successfully passed all tests. This practice enhances reliability and consistency by guaranteeing that only stable and verified images move forward in the deployment pipeline.

Use Docker Content Trust (DCT) to sign and verify images, ensuring they havenâ€™t been tampered with. This practice prevents unauthorized or modified images from being deployed, enhancing the security and integrity of your deployments.





